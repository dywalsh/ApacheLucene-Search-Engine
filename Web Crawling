Web Crawling

	Heterogenous Data
		Multiple media types & languages
		Multimodal indexing and searches
		-identifying the content is a challenge.

	Web search Challenges:
		speed of WWW expansion
		web page update freq
		dynamic page generation
		invisible or deep web and dark web

	Quality of Data
	1 in every 200 words are typo

	SEO (Search enginge optimisation) - Search engine spamming
	-gaming the search engine system


	First step on the IR system - Web crawling
	-finding content to search accross

	content is found via web crawlers
	-browse the web in a methodical manner collating a list of discovered links
	hyperlink

	webcrawler/web robot/web spider
	-> seed urls are provided to the crawler
	-> load page and find every link in the pages source code
	-> every page link the crawler discovers, it adds the page to the queue and searches it for content.
	-> there needs to be a priority in deciding which link to follow next. (FIFO etc)
	-> Need to have a crawling algorithm.

	Retrieving Web Pages
	- URL is subset of URI (you also provide the primary access mechanism)
	- web pages are stored on web servers that use HTTP to exchange information with client software.
	scheme, hostname, resource
	https:// tcd.ie / home

	Web crawler client program connects to dns server
	dns server translates hostname into ip address
	crawl tries to connect to host at that ip address
	**add missing content from slide**

	WEb crawlers spend a lot of time waiting for responses to requests from host servers
	to reduce this inefficiency,web crawlers use threads and fetch hundereds of pages at once,
	however because crawlers could potentially flood web pages with requests for pages.
	avoid this problem, web crawlers use politeness policies
	- e.g delay between requests to same web server.

	Controlling crawling
		crawling a site, even politely, will anger some web server admins, who object to copying of their data (not so much any more as people know the benefit of crawling)
		robots.txt files can be used to control crawlers (can have prioties and allowances for your site, to control crawlers)
		(web crawlers can completely ignore robots.txt)

	Focused crawling
		attempts to download only those pages that are about a particular topic.
		-used by vertical search algos

		rely on the fact that pages about a topic tend to have links to other pages on the same topic.
		-popular pages for a topic are typically used as seeds

		Crawler uses text classifier to decide whether a page is on topic.

	Deep or Invisible Web (challenges crawlers face)
		sites that are difficult for a cralwer to find are reffered to as deep web.
		-potentially much larger than surface web

		Three broad categories:
		-private sites
			no incoming links, or may require log in with valid acc
		-form results
			sites that can be reached only after entering some data in a form
		-scripted pages
		 pages that use JS or Flash, or another client-side languages to generate links (this is changing as crawler tech, tries to mimic user interaction)

	Dark Web 
	is web content that exists on "darknets"

	-overlay networks which use the internet but require specific software configurations or authorizations to access
	-dark web forms small part of deep web

	Sitemaps
	-can link to sitemap in robots.txt file
	-sitemaps contain lists of urls and data about those urls, modification time/freq
	-generated by web server admins
	-tells crawler about pages it might not otherwise find

	gives cralwer a hint about when to check a page for changes.
	(generally basic XML file)




