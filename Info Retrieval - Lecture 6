Info Retrieval - Lecture 6

Indexing / Text Pre-Processing

Indexing 

	-Find content automatically

	Representation
		-Description -> coverage
		-Discrimination -> only show relevant resources

	Indexing Effectiveness:
		-Parameters:
			Index Exhaustivity: degree of description

			Term Specificity: breadth of terms used to describe a document
				-broad terms: retrieve more, but less relevant docs
				-narrow terms: retrieve less, but more relevant docs

	Components of indexing:
		Tokenisation:	breaking a text doc into individual words to be processed.
			-term normalisation: hyphen removal, all lower case
			-stop word removal: remove commonly occuring words.
			-stemming: reducing word variations

		Term Weighting: Deciding of all terms left, which best describe the document content.


		-Tokenisation - Query and IO:
			text processing for queries must be the same as that used for documents.

(First STEP)Term Normalisation:
			Convert doc into stream of text
			Lose context,
			Lose Case
			Lose Punctuation
			Alphabetical ordering of words:

			Hello My name's Dylan -> dylan hello my name s

(Second STEP)Stop word removal:
			-few words appear frequently
			-words not evenly distributed accross docs, language, speeches


			Zipf's Law : 
				the freq of any word is roughly inversely proportional to its rank in the freq table.
				- r * f = k
				- rank * freq ~ Constant

				can be used to define the prob of the occurence of a word 
				-r * Pr =C
				where Pr is the prob of occurence for the r ranked word and c is a constat
				-in english, c ~ 0.1


				Zipf's law tells us which terms are valuable for describing a doc:
					-high freq words are useless (only useful if they form part of a phrase)
					-low freq words may be useless
						-spelling mistakes
						-too rare to be of value
					-middle freq words are the best indexing terms.

				Methods based on zipf's law:
					stop-word removal:
						ignore most freq words.
						commonly used by all search engines.
						-can have domain specific stop word lists (e.g a company indexing docs about a certain medicine, which could be freq occuring in this collection verses the web as a whole)

					significant words (used when it was expensive to have big indexes - e.g expensive processing power):
						ignore most freq and lest freq words (upper and lower cut-offs)
						rarely used.

					Term Weighting:
						commonly used.
						different weights to all terms in doc based on freq of occurence, not just in a doc but in the collection as a whole.

